{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook motivates and provides code for a mixture model designed to distinguish patterns found in the ANE from other regions. This model uses continuous weights to account for the additive influence of a number of factors on distributions of typological features.\n",
    "\n",
    "Of key interest is whether or not a language is in the ANE group. Other factors include family ID and chronological period.\n",
    "\n",
    "In mixture models, it is a common practice to treat the cluster ID as an unknown variable which must be inferred. Here, since we know which languages are in the ANE and which are not, we can treat this variable as an observed variable. Accordingly, the model becomes similar to multinomial logistic regression, although it assumes many response variables for each data point as opposed to just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $\\mathcal{L}$ languages in the sample. Our observed variables consist of\n",
    "\n",
    "* $a_{\\ell}$, the areal group of language $\\ell \\in \\{\\text{ANE},\\text{non-ANE}\\}$\n",
    "* $g_{\\ell}$, the genetic group of language $\\ell \\in \\{1,...,G\\}$ \n",
    "* $t_{\\ell}$, the date at which language $\\ell$ was spoken\n",
    "* $y_{\\ell,d}$ the variant shown by language $\\ell$ for feature $d$\n",
    "\n",
    "Here is the generative process underlying the data:\n",
    "\n",
    "* For each feature index $d \\in \\{1,...,D\\}$ (e.g., Nominal Duality, etc.)\n",
    "  * For each feature variant/level $v \\in \\{1,...,|\\text{feature}_d|\\}$ (e.g., No Duality)\n",
    "    * $\\beta^0_{d,v} \\sim N(0,\\sigma)$ (draw a global bias/intercept for the feature variant)\n",
    "    * $\\beta^{\\text{time}}_{d,v} \\sim N(0,\\sigma)$ (draw a parameter representing the influence of chronology on the feature variant, essentially a slope)\n",
    "    * For each areal group $k \\in \\{\\text{ANE},\\text{non-ANE}\\}$\n",
    "      * $w^a_{k,d,v} \\sim N(0,\\sigma)$ (draw a weight associated with each feature variant in each areal group)\n",
    "    * For each genetic group $k \\in \\{1,...,G\\}$ (e.g., East Semitic)\n",
    "      * $w^g_{k,d,v} \\sim N(0,\\sigma)$ (draw a weight associated with each feature variant in each genetic group)\n",
    "* For each language $\\ell \\in \\{1,...,\\mathcal{L}\\}$\n",
    "  * For each feature index $d \\in \\{1,...,D\\}$\n",
    "    * For each feature variant/level $v \\in \\{1,...,|\\text{feature}_d|\\}$\n",
    "      * $\\psi_{\\ell,d,v} = w^a_{a_{\\ell},d,v} + w^g_{g_{\\ell},d,v} + \\beta^0_{d,v} + \\beta^{\\text{time}}_{d,v}t_{\\ell}$\n",
    "    * $\\boldsymbol \\phi_{\\ell,d} = \\text{softmax} (\\boldsymbol \\psi_{\\ell,d})$\n",
    "    * $y_{\\ell,d} \\sim \\text{Categorical}(\\boldsymbol \\phi_{\\ell,d})$ (draw the observed outcome on the basis of the joint contribution of the language's areal group, genetic group, and date)\n",
    "\n",
    "Above, the standard deviation $\\sigma$ can be fixed or inferred. For simplicity, we can fix it at one. Note that with a small amount of effort, we can use Gaussian distributions with non-diagonal covariance (as opposed to the diagonal covariance assumed here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a proof of concept using the current data. For lack of a comparison level, we can treat Egyptian as non-ANE and treat all of the variables as categorical (when some perhaps can be recoded as binary), NAs should be removed, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "theano.config.gcc.cxxflags = \"-fbracket-depth=100000\"\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "with open('../features.csv','r') as f:\n",
    "    for row in csv.reader(f,delimiter=','):\n",
    "        features.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_data_raw = [l[7:-2] for l in features]\n",
    "lang_raw = [l[:3] for l in features]\n",
    "fam_raw = [l[3] for l in features]\n",
    "area_raw = [l[4] for l in features]\n",
    "date_raw = [l[-2:] for l in features]\n",
    "\n",
    "fams = sorted(set([l for l in fam_raw[1:]]))\n",
    "G = len(fams)\n",
    "\n",
    "N = len(ling_data_raw)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_vars = defaultdict(list)\n",
    "for l in ling_data_raw[1:]:\n",
    "    for i,f in enumerate(l):\n",
    "        feat_vars[ling_data_raw[0][i]].append(f)\n",
    "\n",
    "for k in feat_vars.keys():\n",
    "    feat_vars[k] = sorted(set(feat_vars[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [len(feat_vars[k]) for k in feat_vars.keys()]\n",
    "X = sum(R)\n",
    "feat_var_list = [(k,v) for k in feat_vars.keys() for v in feat_vars[k]]\n",
    "\n",
    "S = [[sum(R[:d]),sum(R[:d])+R[d]] for d in range(len(R))] #the indices for each distribution in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_var_id = np.zeros([N,X])\n",
    "fam_id = np.zeros([N,G])\n",
    "area_id = np.zeros([N,2])\n",
    "date_id = np.zeros([N,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    for j in range(len(feat_vars.keys())):\n",
    "        feat_var_id[i,feat_var_list.index((ling_data_raw[0][j],ling_data_raw[i+1][j]))] = 1.\n",
    "    fam_id[i,fams.index(fam_raw[i+1])] = 1.\n",
    "    if lang_raw[i+1][0] == 'Egyptian':\n",
    "        area_id[i,1] = 1.\n",
    "    else:\n",
    "        area_id[i,0] = 1.\n",
    "    date = date_raw[i+1]\n",
    "    #too lazy to deal with BCE/CE\n",
    "    date_id[i,0] = np.mean([float(d.split()[0]) for d in date]) + 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglik(phi):\n",
    "    def llik(feat_var_id):\n",
    "        lliks = tt.log(phi)*feat_var_id\n",
    "        return(tt.sum(lliks))\n",
    "    return(llik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.Model()\n",
    "with model:\n",
    "    beta_0 = tt.stack([pm.Normal('beta_0_{}'.format(x),0,1) for x in range(X)])\n",
    "    beta_time = tt.stack([[pm.Normal('beta_time_{}'.format(x),0,1) for x in range(X)]])\n",
    "    w_a = tt.stack([[pm.Normal('w_a_{}_{}'.format(k,x),0,1) for x in range(X)] for k in range(2)]) #areal group prior\n",
    "    w_g = tt.stack([[pm.Normal('w_g_{}_{}'.format(k,x),0,1) for x in range(X)] for k in range(G)]) #genetic group prior\n",
    "    psi = beta_0 + tt.dot(date_id,beta_time) + tt.dot(area_id,w_a) + tt.dot(fam_id,w_g)\n",
    "    phi = tt.concatenate([tt.nnet.softmax(psi[:,S[d][0]:S[d][1]]) for d in range(len(R))],-1)\n",
    "    target = pm.DensityDist('target',loglik(phi=phi),observed=dict(feat_var_id=feat_var_id))\n",
    "    inference = pm.ADVI()\n",
    "    inference.fit(100000, obj_optimizer=pm.adam(learning_rate=.01,beta1=.8),callbacks=[pm.callbacks.CheckParametersConvergence()])\n",
    "    trace = inference.approx.sample()\n",
    "    posterior = {k:trace[k] for k in trace.varnames if not k.endswith('__')}\n",
    "    posterior['ELBO'] = inference.hist\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
